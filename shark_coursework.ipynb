{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shark-coursework.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0iq3u7srm0F_",
        "x1bqqtBWm6cV",
        "lw-IR7KXnAZm",
        "yUTlBlArnDYd"
      ],
      "authorship_tag": "ABX9TyMOxYo49Q3DqgoU3xHURVpx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GalbaKor/AI-shark-GAN-coursework/blob/main/shark_coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coursework - generating images to improve CNN classification\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tsEMo-8o1CAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - baseline MLP results\n",
        "\n",
        "Step 2 - baseline CNN results (shark vs fish then shark x4)\n",
        "\n",
        "Step 3 - DCGAN image generation\n",
        "\n",
        "Step 4 - StyleGAN2 image generation\n",
        "\n",
        "Step 5 - Retesting CNN with StyleGAN2 generated images"
      ],
      "metadata": {
        "id": "Mo2XdazX1In3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRE766DBgr8B"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0iq3u7srm0F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "kKnvO6Uhp-An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imshow function for displaying images\n",
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.5, 0.5, 0.5])\n",
        "        std = np.array([0.5, 0.5, 0.5])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "Zb2aDtELp-gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and splitting my model into Train, Validation and Test subcategories"
      ],
      "metadata": {
        "id": "OuR_w5hqqeTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip dataset folder \n",
        "!unzip '/content/gdrive/MyDrive/UWE_AI_CT/Datasets/CNN_coursework_real_dataset.zip' > /dev/null"
      ],
      "metadata": {
        "id": "v4FGOExtp_dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install split-folders"
      ],
      "metadata": {
        "id": "ZBQAX3iuqDke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio(\"/content/CNN_coursework_real_dataset\", output=\"coursework_real_dataset\", seed=1111, ratio=(.7, .15, .15), group_prefix=None)"
      ],
      "metadata": {
        "id": "t1-yrViWqEiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset directory\n",
        "data_dir = '/content/coursework_real_dataset'\n",
        "\n",
        "# define batch size\n",
        "batch_size = 64\n",
        "\n",
        "# # define transforms (grayscale)\n",
        "# transform_gs = transforms.Compose([transforms.Grayscale(), # convert colour images to greyscale\n",
        "#                                 transforms.Resize(28), # resize to 28x?\n",
        "#                                 transforms.CenterCrop(28), # take a square (28x28) crop from the centre\n",
        "#                                 transforms.ToTensor(), # convert data to torch.FloatTensor\n",
        "#                                 transforms.Normalize(0.5, 0.5)]) # normalise with mean 0.5 and standard deviation 0.5\n",
        "\n",
        "# define transforms (colour)\n",
        "transform = transforms.Compose([transforms.Resize(28), # resize to 28x?\n",
        "                                transforms.CenterCrop(28), # take a square (28x28) crop from the centre\n",
        "                                transforms.ToTensor(), # convert data to torch.FloatTensor\n",
        "                                transforms.Normalize([0.5, 0.5, 0.5],\n",
        "                                                     [0.5, 0.5, 0.5])]) # normalise with mean 0.5 and standard deviation 0.5 for each colour channel\n",
        "\n",
        "# choose the training, validation and test datasets\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=transform)\n",
        "val_data = datasets.ImageFolder(data_dir + '/val', transform=transform)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=transform)\n",
        "\n",
        "# prepare the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "bakVBWm9qF3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing showing a batch of images to make sure things are working"
      ],
      "metadata": {
        "id": "MONNuQ7Rqm2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of training images\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "print(labels.shape)\n",
        "\n",
        "# plot the first 4 images in the batch, along with the corresponding labels\n",
        "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
        "for ii in range(4):\n",
        "    ax = axes[ii]\n",
        "    imshow(images[ii], ax=ax, normalize=True)\n",
        "    ax.set_title(str(labels[ii].item())) \n",
        "# 0 is a fish, #1 is a shark"
      ],
      "metadata": {
        "id": "234scCuDqHZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the network\n"
      ],
      "metadata": {
        "id": "CNLBtnZBqLBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if using greyscale images, image size 28x28x1 = 784\n",
        "# if using colour images, image size 28x28x3 = 2352\n",
        "input_size = 2352\n",
        "                                                                   \n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the NN architecture\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # number of hidden nodes in each layer\n",
        "        hidden_1 = 1028\n",
        "        hidden_2 = 512\n",
        "        hidden_3 = 256\n",
        "        numberOfClasses = 5\n",
        "        # linear layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_1)\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
        "        self.fc4 = nn.Linear(hidden_3, numberOfClasses) \n",
        "        # Dropout module with 0.2 drop probability - to prevent overfitting\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(-1, input_size)\n",
        "        # add hidden layers, with relu activation function, and dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "        # add output layer with logSoftmax\n",
        "        x = F.log_softmax(self.fc4(x), dim=1) \n",
        "        return x\n",
        "\n",
        "# initialise the NN\n",
        "model = Classifier()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "c6tWdfwdqMxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify loss function (negative log likelihood loss)\n",
        "criterion = nn.NLLLoss()\n",
        "# specify optimizer (Adam optimiser) and learning rate = 0.003\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)"
      ],
      "metadata": {
        "id": "JfbYcEgRqN1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the network\n",
        "\n",
        "5 epochs were used as the MLP was never intended to be the focus of the coursework"
      ],
      "metadata": {
        "id": "M-5Co0qLqOq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 5\n",
        "\n",
        "# initialise tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "# create empty lists to store the training and validation losses\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train() # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval() # prep model for evaluation\n",
        "    for data, target in val_loader:\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update running validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(val_loader.sampler)\n",
        "\n",
        "    # store the training and validation losses for later visualisation\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(valid_loss)\n",
        "    \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        train_loss,\n",
        "        valid_loss\n",
        "        ))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'MLP_model_3.pt') # save in colab\n",
        "        torch.save(model.state_dict(), '/content/gdrive/MyDrive/MLP_coursework_1.pt') # save in google drive\n",
        "        valid_loss_min = valid_loss"
      ],
      "metadata": {
        "id": "Q-l1mVTcqQDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualising results"
      ],
      "metadata": {
        "id": "HP0FyX4UqRVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the training and validation losses over time\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(val_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "metadata": {
        "id": "WUO2gQp0qSNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Model with the Lowest Validation Loss\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/MLP_coursework_1.pt'))"
      ],
      "metadata": {
        "id": "zQfW5_dEqTNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() # prep model for evaluation\n",
        "\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(len(target)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.sampler)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(2):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "id": "ca3zeAcWqT1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Sample Test Results\n",
        "\n",
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds = torch.max(output, 1)\n",
        "\n",
        "# plot the first 4 images in the batch, along with the corresponding labels\n",
        "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
        "for ii in range(4):\n",
        "    ax = axes[ii]\n",
        "    imshow(images[ii], ax=ax, normalize=True)\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[ii].item()), str(labels[ii].item())),\n",
        "                 color=(\"green\" if preds[ii]==labels[ii] else \"red\")) \n",
        "    \n",
        "#0 = fish\n",
        "#1 = shark"
      ],
      "metadata": {
        "id": "ejO8XDW6qU39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "x1bqqtBWm6cV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN first testing phase**\n",
        "\n",
        "I ran through two different tests\n",
        "\n",
        "One using a dataset of fish vs sharks\n",
        "\n",
        "Another using a dataset of 4 different types of sharks"
      ],
      "metadata": {
        "id": "Iv32jRTKqzq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print('CUDA is not available. Training on CPU.')\n",
        "else:\n",
        "  print('CUDA is available. Training on GPU.')"
      ],
      "metadata": {
        "id": "uywzbSYAq1Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip dataset folder \n",
        "!unzip '/content/gdrive/MyDrive/UWE_AI_CT/Datasets/CNN_coursework_real_dataset.zip' > /dev/null"
      ],
      "metadata": {
        "id": "RyaNLJSmrtlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First and second run used a real dataset\n",
        "\n",
        "Retesting after GAN generation used a fake + real dataset"
      ],
      "metadata": {
        "id": "0SkqTk4gtJwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio(\"/content/CNN_coursework_real_dataset\", output=\"/content/CNN_coursework_real_dataset_split\", seed=1111, ratio=(.7, .15, .15), group_prefix=None)"
      ],
      "metadata": {
        "id": "uN29nk5SrvMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio(\"/content/CNN_fake_dataset_sharks\", output=\"shark_dataset_split\", seed=1111, ratio=(.7, .15, .15), group_prefix=None)"
      ],
      "metadata": {
        "id": "6d5cNUU8tIFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset directory\n",
        "data_dir = '/content/shark_dataset_split'\n",
        "\n",
        "# define batch size\n",
        "batch_size = 16\n",
        "\n",
        "# define transforms\n",
        "# training (with random flips and rotation)\n",
        "train_transform = transforms.Compose([transforms.Resize(236), # resize to 236x?\n",
        "                                transforms.RandomRotation(15), # random rotation\n",
        "                                transforms.CenterCrop(224), # take a square (224x224) crop from the centre\n",
        "                                transforms.RandomHorizontalFlip(), # randomly flip on horizontal axis\n",
        "                                transforms.ToTensor(), # convert data to torch.FloatTensor\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                     [0.229, 0.224, 0.225])]) # normalise for each colour channel\n",
        "\n",
        "# validation and testing\n",
        "transform = transforms.Compose([transforms.Resize(224), # resize to 224x?\n",
        "                                transforms.CenterCrop(224), # take a square (224x224) crop from the centre\n",
        "                                transforms.ToTensor(), # convert data to torch.FloatTensor\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                     [0.229, 0.224, 0.225])]) # normalise for each colour channel\n",
        "\n",
        "# choose the training, validation and test datasets\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transform)\n",
        "val_data = datasets.ImageFolder(data_dir + '/val', transform=transform)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=transform)\n",
        "\n",
        "# prepare the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "e0UKq8ntrwMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "# using a GoogLeNet pretrained network\n",
        "# #########################################################################\n",
        "# NB This network requires an image input size of 3x224x224 and normalised using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "model = models.googlenet(pretrained=True)\n",
        "\n",
        "# get the number of inputs for the final layer (fc) of the network\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "# replace the final layer so that the output is number of classes\n",
        "# this was 2 in the first run and 4 in the second and last\n",
        "model.fc = nn.Linear(num_ftrs, 4)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "W-rNJe7crxZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function (cross entropy loss)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# specify optimiser (Adam optimiser) and learning rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "IjNa2oC-r9XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 50\n",
        "\n",
        "# initialise tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "# create empty lists to store the training and validation losses\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train() # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval() # prep model for evaluation\n",
        "    for data, target in val_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update running validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(val_loader.sampler)\n",
        "\n",
        "    # store the training and validation losses for later visualisation\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(valid_loss)\n",
        "    \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        train_loss,\n",
        "        valid_loss\n",
        "        ))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'resnet18_model_fine_tune_aug_shark_googlenet_CW_1.pt') # save in colab\n",
        "        torch.save(model.state_dict(), '/content/gdrive/MyDrive/UWE_AI_CT/Models/resnet18_model_fine_tune_aug_shark_googlenet_CW_1.pt') # save in google drive\n",
        "        valid_loss_min = valid_loss"
      ],
      "metadata": {
        "id": "Z8dAmeegr9uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise the training and validation losses over time\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(val_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "metadata": {
        "id": "LZ0PGqjmsAE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Model with the Lowest Validation Loss\n",
        "model.load_state_dict(torch.load('/content/resnet18_model_fine_tune_aug_shark_googlenet_CW_1.pt'))"
      ],
      "metadata": {
        "id": "hnwMG0pvsAfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the Trained Network\n",
        "\n",
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() # prep model for evaluation\n",
        "\n",
        "for data, target in test_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(len(target)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.sampler)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "# change value here to number of classes\n",
        "for i in range(4):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "id": "qkiKI3hOsCEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Sample Test Results\n",
        "\n",
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if train_on_gpu:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the first 4 images in the batch, along with the corresponding labels\n",
        "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
        "for ii in range(4):\n",
        "    ax = axes[ii]\n",
        "    imshow(images.cpu()[ii], ax=ax, normalize=True)\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[ii].item()), str(labels[ii].item())),\n",
        "                 color=(\"green\" if preds[ii]==labels[ii] else \"red\")) \n",
        "    \n",
        "#First run with shark vs fish dataset\n",
        "#0 is a fish\n",
        "#1 is a shark\n",
        "\n",
        "#second run with tiger vs bull vs great white vs oceanic whitetip\n",
        "#0 is bull\n",
        "#1 is great white\n",
        "#2 is oceanic whitetip\n",
        "#3 is tiger"
      ],
      "metadata": {
        "id": "Y0H3PIWjsCy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DCGAN\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lw-IR7KXnAZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "#manualSeed = 999\n",
        "manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "metadata": {
        "id": "eImTdrShtX84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 32\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 500\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "metadata": {
        "id": "vRvOtgJdta8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading my dataset here\n",
        "\n",
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "w0mtaNqPtbsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip dataset folder \n",
        "!unzip '/content/gdrive/MyDrive/UWE_AI_CT/Datasets/shark_dataset_split_dcgan.zip' > /dev/null"
      ],
      "metadata": {
        "id": "VJwf8cRqtctl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip dataset folder \n",
        "!unzip '/content/gdrive/MyDrive/UWE_AI_CT/Datasets/shark_dataset_3.zip' > /dev/null"
      ],
      "metadata": {
        "id": "n26UQZLatd2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset directory\n",
        "data_dir = '/content/shark_dataset_split_dcgan/bull'\n",
        "# the data_dir must contain another folder with the images inside"
      ],
      "metadata": {
        "id": "VxaRMCUvtf16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset\n",
        "dataset = dset.ImageFolder(root=data_dir,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "\n",
        "# Run on gpu if available\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "metadata": {
        "id": "XYf-e4D8tgpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "Z07UoaKVthoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Code 64x64 images\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "UMR7M9iMtiO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "metadata": {
        "id": "e4CxaLTJtj0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Code\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "PbBOymI8tkgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    \n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "metadata": {
        "id": "sdIlwkdYtl02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "metadata": {
        "id": "D8DSH6FXtmdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Add the gradients from the all-real and all-fake batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "        \n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        \n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 200 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            \n",
        "        iters += 1"
      ],
      "metadata": {
        "id": "oORLLhROtnLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YIGzXUnftox4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "hhFaMyk2tqK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xt9PZZV_trpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StyleGAN2\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yUTlBlArnDYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1 Installing Repository**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Installs the StyleGAN repository or sets the directory if already installed.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rQxEzrBgnNI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\")"
      ],
      "metadata": {
        "id": "FXpBTqfCnKYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n",
        "    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir colab-sg2-ada-pytorch\n",
        "    %cd colab-sg2-ada-pytorch\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    %cd pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n",
        "    %cd ../\n",
        "\n",
        "!pip install ninja opensimplex torch==1.7.1 torchvision==0.8.2"
      ],
      "metadata": {
        "id": "pimRLRgfnKvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run once at the start to update if necessary"
      ],
      "metadata": {
        "id": "KORXjEOinm-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "!git pull\n",
        "!git stash\n",
        "!git checkout origin/main -- train.py generate.py legacy.py closed_form_factorization.py flesh_digression.py apply_factor.py README.md calc_metrics.py training/stylegan2_multi.py training/training_loop.py util/utilgan.py"
      ],
      "metadata": {
        "id": "RkkscTF_nl6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 Dataset Preparation**\n",
        "\n",
        "Make a directory in the stylegan2 dataset folder.\n",
        "Then, run the dataset creation tool with desired height and widths to convert the zipped dataset to the correct format."
      ],
      "metadata": {
        "id": "ZF1ZtgSBntBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('./datasets/whitetips')"
      ],
      "metadata": {
        "id": "-k4bM_J_n39R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/dataset_tool.py --height 256 --width 256 --source /content/drive/MyDrive/UWE_AI_CT/Datasets/oceanic_whitetips.zip --dest /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/whitetips"
      ],
      "metadata": {
        "id": "wugc1pVJnr9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point a pretrained model was added to the \"stylegan2-ada-pytorch/pretrained\" folder"
      ],
      "metadata": {
        "id": "QiMecux8oKJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 Train Model**\n",
        "\n",
        "Dataset set to be whitetips.\n",
        "\n",
        "*   Resume from was first set to be the pretrained model.\n",
        "*   Mirror X and Y initially turned on.\n",
        "*   Gamma values initially set to be 50.\n",
        "*   Augs set to 'bg' initially. 'bgcfnc' returned worse results.\n",
        "*   Snapshot value changed between 4-10 to determine train times."
      ],
      "metadata": {
        "id": "c8Jn2V6ToQ5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resume_path first set to be pretrained\n",
        "# 2nd and further training runs were changed to the latest pkl file\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/whitetips'\n",
        "resume_from = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00017-oceanic_whitetips-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000520.pkl'\n",
        "\n",
        "#aug strength needs changing after each run to the latest result found in log txt file\n",
        "aug_strength = 0.726\n",
        "train_count = 632\n",
        "\n",
        "# mirror x and y values help accomodate for smaller datasets but y value may have caused issues\n",
        "mirror_x = True\n",
        "mirror_y = False\n",
        "\n",
        "#gamma value set to 50 initially. Further testing produced worse results at 10\n",
        "gamma_value = 50.0\n",
        "\n",
        "#augs set to bg initially. full augmentation worsened FID values\n",
        "augs = 'bgcfnc'\n",
        "config = '11gb-gpu'\n",
        "\n",
        "#snapshot set to 4 to test training times, then increased to 8 when colab runtimes limits were found\n",
        "snapshot_count = 8"
      ],
      "metadata": {
        "id": "wgTn2n1CoruY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, metrics were turned off. I studied the stylegan2 documentation to find the FID and PR metrics, then added FID to the train.py function to generate FID at each snapshot"
      ],
      "metadata": {
        "id": "iq2_1lcQpNJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --gpus=1 --cfg=$config --metrics=fid50k_full --outdir=./results --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"
      ],
      "metadata": {
        "id": "y8whakdtpMMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively you can manually generate metrics by running these commands and picking a specific pkl file"
      ],
      "metadata": {
        "id": "QyeAOxHApXRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python calc_metrics.py --metrics=pr50k3_full \\\n",
        "    --network=#pklfilelocation"
      ],
      "metadata": {
        "id": "CJoH5ANApb5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python calc_metrics.py --metrics=fid50k_full \\\n",
        "    --network=#pklfilelocation"
      ],
      "metadata": {
        "id": "mPDAM-C5pc0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 Image Generation for use in datasets**\n",
        "\n",
        "I left these at the default, apart from changing the seeds to be 1-300 to generate 300 images.\n",
        "\n",
        "Generated images from the 520 kimg mark, the last point at the time."
      ],
      "metadata": {
        "id": "x2k41Dq0pe0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate.py --outdir=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/out/whitetips_520kimg --trunc=0.5 --seeds=1-300 --network='/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00017-oceanic_whitetips-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000520.pkl'"
      ],
      "metadata": {
        "id": "ciEo5z36pvkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}